import pandas as pd
import optuna
import mlflow
import joblib

from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score,balanced_accuracy_score

from Attrition_App import logger
from pycaret.classification import ClassificationExperiment
from Attrition_App.constants import *
from Attrition_App.entity.config_entity import ModelSelectionConfig


# ModelsConfig contains the configuration for different machine learning models,
# including model names and hyperparameters, which can be used for initializing and training models.
class ModelsConfig:
    def __init__(self,X,y):
        self.models_dict = {
               'LogisticRegression': LogisticRegression,
               'RandomForestClassifier': RandomForestClassifier, 
               'DecisionTreeClassifier': DecisionTreeClassifier,
               'SGDClassifier':SGDClassifier,
               'GradientBoostingClassifier':GradientBoostingClassifier
               }
        
        self.X = X
        self.y = y

    # This method evaluates the model's performance by calculating the ROC AUC score and balanced accuracy score.
    def evaluate(self,actual,prediction):
        """
        Evaluates the model's performance using two metrics: ROC AUC and balanced accuracy score.

        The method takes the actual values and predicted values as input, calculates the ROC AUC score 
        to evaluate the model's ability to distinguish between classes, and calculates the balanced accuracy 
        score to evaluate the model's performance on imbalanced datasets.

        Args:
            actual (array-like): The true labels of the dataset.
            prediction (array-like): The predicted labels generated by the model.

        Returns:
            dict: A dictionary containing the ROC AUC score and the balanced accuracy score.
        """
        return {
                'roc_score':roc_auc_score(actual,prediction),
                'balance_accuracy_score':balanced_accuracy_score(actual,prediction)
                }

    # This method defines multiple objective functions for different classifiers used in hyperparameter optimization,
    # which can be utilized in optimization frameworks like Optuna.
    def get_objective(self):  
        """
        Defines and returns the objective functions for different machine learning classifiers.

        Each objective function is responsible for hyperparameter tuning and evaluation using ROC AUC score for a specific model.
        These functions are designed to work with an optimization framework (e.g., Optuna), which optimizes the hyperparameters.
        
        The following classifiers are included:
        - LogisticRegression
        - RandomForestClassifier
        - SGDClassifier
        - DecisionTreeClassifier
        - GradientBoostingClassifier

        Args:
            None

        Returns:
            dict: A dictionary where keys are classifier names and values are their corresponding objective functions.
        """  

        def LogisticRegObj(trial):
            lr_C = trial.suggest_uniform('C', 0.0, 1.0)

            classifier_obj = LogisticRegression(C=lr_C)
            classifier_obj = classifier_obj.fit(self.X,self.y)
            pred = classifier_obj.predict(self.X)
            score = roc_auc_score(self.y,pred)
            return score

        def RandomForObj(trial):
            rfc_n_estimators = trial.suggest_int('n_estimators', 50, 250, 25)
            rfc_criterion = trial.suggest_categorical("criterion", ["gini", "entropy", "log_loss"])
            rfc_min_samples_split  = trial.suggest_uniform('min_samples_split',0.2, 1.0)
            rfc_max_depth = trial.suggest_int('max_depth', 2, 10, 2)

            classifier_obj = RandomForestClassifier(
                                            n_estimators=rfc_n_estimators,
                                            criterion=rfc_criterion,
                                            min_samples_split = rfc_min_samples_split,
                                            max_depth = rfc_max_depth
                                            )
            classifier_obj = classifier_obj.fit(self.X,self.y)
            
            pred = classifier_obj.predict(self.X)
            score = roc_auc_score(self.y,pred)

            return score

        def SGDRegObj(trial):
            sgd_eta0 = trial.suggest_uniform('eta0',0.1, 1.0)

            sgd_loss = trial.suggest_categorical('loss', ['hinge',
                                                            'log_loss',
                                                            'modified_huber',
                                                            'squared_hinge',
                                                            'perceptron',
                                                            'squared_error',
                                                            'huber'])
            sgd_learning_rate = trial.suggest_categorical('learning_rate', ['constant',
                                                            'optimal',
                                                            'invscaling',
                                                            'adaptive'])

            classifier_obj = SGDClassifier(loss=sgd_loss,
                                            eta0 = sgd_eta0,
                                            learning_rate=sgd_learning_rate)

            classifier_obj = classifier_obj.fit(self.X,self.y)
            pred = classifier_obj.predict(self.X)
            score = roc_auc_score(self.y,pred)

            return score

        def DecisionTreeClassObj(trial):
            criterion = trial.suggest_categorical("criterion", ["log_loss", "gini",'entropy'])
            max_depth = trial.suggest_int(
                "n_estimators", 3, 8, 1
            )
            
            min_samples_leaf = trial.suggest_float(
                "min_samples_leaf", 0.3, 0.9, log=True
            )

            min_samples_split = trial.suggest_float(
                "min_samples_split", 0.3, 0.9, log=True
            )
            
            classifier_obj = DecisionTreeClassifier(
                                                criterion = criterion,
                                                max_depth=max_depth,  
                                                min_samples_split = min_samples_split,
                                                min_samples_leaf=min_samples_leaf
                                                )

            classifier_obj = classifier_obj.fit(self.X,self.y)
            pred = classifier_obj.predict(self.X)
            score = roc_auc_score(self.y,pred)

            return score


        def GradientBoostClassObj(trial):
            loss = trial.suggest_categorical("loss", ["log_loss", "exponential"])
            max_depth = trial.suggest_int(
                    "n_estimators", 3, 8, 1
                )
            learning_rate = trial.suggest_float(
                    "learning_rate", 0.0005, 0.05, log=True
                )
            n_estimators = trial.suggest_int(
                    "n_estimators", 50, 350, 50
                )
            
            subsample = trial.suggest_float(
                    "subsample", 0.3, 0.9, log=True
                )
            
            min_samples_leaf = trial.suggest_float(
                    "min_samples_leaf", 0.3, 0.9, log=True
            )

            min_samples_split = trial.suggest_float(
                    "min_samples_split", 0.3, 0.9, log=True
            )
            
            classifier_obj = GradientBoostingClassifier(
                                                n_estimators = n_estimators,
                                                max_depth=max_depth,
                                                loss=loss,
                                                learning_rate = learning_rate,
                                                subsample=subsample,
                                                min_samples_split=min_samples_split,
                                                min_samples_leaf=min_samples_leaf
                                                )

            classifier_obj = classifier_obj.fit(self.X,self.y)
            pred = classifier_obj.predict(self.X)
            score = roc_auc_score(self.y,pred)

            return score

        objectives = {
            'LogisticRegression':LogisticRegObj,
            'RandomForestClassifier': RandomForObj,
            'SGDClassifier': SGDRegObj,
            'GradientBoostingClassifier':GradientBoostClassObj,
            'DecisionTreeClassifier': DecisionTreeClassObj
        }    

        return objectives


class ModelSelection:
    def __init__(self,model_selection_config=ModelSelectionConfig):
        self.model_selection_config = model_selection_config

    # This method loads the transformed training data (X and y) from a specified directory 
    # for model selection and assigns them to the class attributes self.X and self.y.
    def load_data(self):
        """
        Loads the transformed data for model selection.

        This method constructs the file paths for the transformed feature and target datasets (train_x and train_y)
        based on the provided configuration, reads them into Pandas DataFrames, and assigns them to 
        the class attributes self.X and self.y.

        It uses the paths defined in the model selection configuration to locate the files.

        If an error occurs during the data loading process, it logs a warning.

        Args:
            None

        Returns:
            None
        """
        root = Path('/').resolve()
        file_directory = self.model_selection_config.data_transform_dir
        try:
            logger.info('Extracting transformed data for model selection.')
            train_x_path = Path.joinpath(root,file_directory,MODEL_SELECTION_TRAIN_X)
            train_y_path = Path.joinpath(root,file_directory,MODEL_SELECTION_TRAIN_Y)
            self.X = pd.read_csv(train_x_path,header=None),
            self.y = pd.read_csv(train_y_path,header=None) 
        except Exception as e:
            logger.warning('Extraction of transformed data failed.')

    # This method preprocesses the feature data (X) and target data (y) by performing some basic cleaning,
    # data transformation, and prepares them for model training. 
    def preprocess_data(self):
        """
        Preprocesses the data by performing cleaning and feature transformation.

        This method:
        - Fills missing values in the feature dataset (X) with 0.
        - Creates a copy of the features to use for further processing.
        - Adds the target data (y) as a new column to the feature dataset.
        - Renames columns of the feature dataset to ensure they are strings.
        - Initializes the ModelsConfig with the cleaned features and target data.

        Args:
            None

        Returns:
            None
        """
        self.X = self.X[0]
        self.X.fillna(0,inplace=True)
        self.data = self.X.copy()

        self.data[MODEL_SELECTION_TARGET] = self.y[1]
        self.data['target'] = self.y[1]

        self.X.columns = [str(feature) for feature in self.X.columns]
        self.y = self.y[1]

        self.models_config = ModelsConfig(self.X,self.y)


    # This method runs an experiment to fit and compare multiple classification models using a predefined setup.
    # It then stores the top models based on the performance from the comparison.
    def experiment_different_models(self):
        """
        Runs an experiment to fit and compare different classification models, 
        and stores the top-performing models based on evaluation metrics.

        This method:
        - Sets up a classification experiment using the provided data and target.
        - Compares multiple models by cross-validation and ranks them.
        - Selects the top models based on the specified number of top models to retain.
        - Stores the selected top models in the self.top_models list.
        - Logs the completion status and the names of the top models.

        Args:
            None

        Returns:
            None
        """
        exp = ClassificationExperiment()

        try:
            logger.info(f'Fitting different models started')
            exp.setup(self.data, target = MODEL_SELECTION_TARGET, session_id = 125)

            top_models = exp.compare_models(include=MODEL_SELECTION_MODELS_LIST,
                                            cross_validation=True,
                                            fold=MODEL_SELECTION_FOLDS,
                                            n_select=MODEL_SELECTION_TOP_SELECTS)
            
            self.top_models = [self.models_config.models_dict[model.__class__.__name__] for model in top_models]
            logger.info(f'Fitting different models completed successfully')
            logger.info(f'top {MODEL_SELECTION_TOP_SELECTS} models are:')
            for model in self.top_models:
                logger.info(model().__class__.__name__ )
        except Exception as e:
            logger.warning('Fitting different models failed')

    # This method performs hyperparameter tuning for the top models selected earlier using Optuna,
    # logs the results with MLflow, and stores details of the models, their scores, and hyperparameters.
    def hyperparameter_tuning_top_models(self):
        """
        Performs hyperparameter tuning for the top models using Optuna, logs the results with MLflow, 
        and stores model details, hyperparameters, and evaluation metrics.

        This method:
        - Sets the MLflow tracking URI for experiment logging.
        - Iterates through the top models and performs hyperparameter tuning using Optuna.
        - Logs the best hyperparameters and model scores to MLflow.
        - Visualizes tuning results through contour plots, parallel coordinate plots, and optimization history.
        - Stores the tuned model, its best hyperparameters, and associated performance in the `self.models_detils` dictionary.

        Args:
            None

        Returns:
            None
        """

        mlflow.set_tracking_uri(uri='http://127.0.0.1:1500')
        self.models_detils = {}

        for index,model in enumerate([model for model in self.top_models]):
            model_name = model().__class__.__name__
            experiment = mlflow.set_experiment(experiment_name=f'experiement_{model_name}')

            objectives = self.models_config.get_objective()
            evaluate = self.models_config.evaluate

            study = optuna.create_study(direction="maximize")
            study.optimize(objectives[model_name], n_trials=200)

            params = study.best_params
            self.models_detils[model_name] =  {'score':study.best_value,
                                                        'params':study.best_params,
                                                        'model':None}
            params_list = list(study.best_params.keys())
            plot_contour_fig = optuna.visualization.plot_contour(study, params=params_list)
            plot_parallel_coordinate_fig = optuna.visualization.plot_parallel_coordinate(study, params=params_list)
            plot_optimization_history_fig = optuna.visualization.plot_optimization_history(study)

            with mlflow.start_run(experiment_id=experiment.experiment_id):
                model = model(**params)
                model.fit(self.X,self.y)
                self.models_detils[model_name]['model'] = model
                predict = model.predict(self.X)

                mlflow.log_param("models score", study.best_value)
                mlflow.log_metrics(evaluate(self.y,predict))
                mlflow.log_params(model.get_params())
                mlflow.log_figure(plot_contour_fig, f"plot_contour_fig_{model_name}.png")
                mlflow.log_figure(plot_parallel_coordinate_fig, f"plot_parallel_coordinate_fig_{model_name}.png")
                mlflow.log_figure(plot_optimization_history_fig, f"plot_optimization_history_fig_{model_name}.png")        

    def store_best_performing_model(self):
        """
        Stores the best-performing model (based on the score) to the specified directory.

        This method:
        - Sorts the models based on their performance score in descending order.
        - Selects the best model (the one with the highest score).
        - Serializes the selected model using `joblib` and saves it to the specified directory.
        - Logs the process and completion of the model storage.

        Args:
            None

        Returns:
            None
        """
        logger.info('Storing best model')
        models = sorted(self.models_detils.items(),key=lambda item:item[1]['score'],reverse=True)
        best_model = [model[-1]['model'] for model in models][0]

        root = Path('/').resolve()
        model_dir = self.model_selection_config.best_models_dir
        
        file_name = Path.joinpath(root,model_dir,MODEL_SELECTION_BEST_MODEL)
        joblib.dump(best_model, file_name)

        logger.info('Storing best model completed successfully')


    def initialize_model_selection(self):
        self.load_data()    
        self.preprocess_data()
        self.experiment_different_models()
        self.hyperparameter_tuning_top_models()
        self.store_best_performing_model()